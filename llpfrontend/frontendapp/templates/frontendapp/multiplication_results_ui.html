<div>
  <div id="task_details">
    <label>Equation (Input code: {{input_code}}): {{equation}}</label>
    <labe>The correct answer: {{answer}}</label>
    <br/>
  </div>
  <b>Specific model results:</b>
  <div id="results_per_model">
    {% for readable_name, prompt_and_counts in prompt_and_interpreted_output_counts_per_readable_llm_config_combination.items %}
      {% if prompt_and_counts.counts|length > 0 %}
        <label>Iterpreted outputs for model {{readable_name}}:</label>
        <pre>Prompt:<p>{{prompt_and_counts.prompt}}</p></pre>
        <ul>
        {% for answer, count in prompt_and_counts.counts.items %}
          <li>Answer "{{answer}}" - answered {{count}} times</li>
        {% endfor %}
        </ul>
      {% else %}
        <label>There are no outputs for model {{readable_name}}</label>
      {% endif %}
    {% endfor %}
  </div>
</div>
