"""
This file defines the classes and methods necessary for constructing prompts for the LLMs
"""

import json
import logging
import os
import string
from typing import Any
import tiktoken
from typing import *
from transformers import AutoTokenizer
from enum import Enum

from .run_config import Config
from .task_type import TaskType, new_tokens_limit_per_task_type_int, task_type_int_to_str
from .runnable_model_data import RunnableModel

log = logging.getLogger(os.path.basename(__file__))
logging.basicConfig(level=logging.INFO)


alphabet2idx = {letter:i for i, letter in enumerate(string.ascii_uppercase)}
idx2alphabet = {i:letter for i, letter in enumerate(string.ascii_uppercase)}


class UniversalTokenizer:
  """This class is an adapter for different tokenizers, it selects the appropriate tokenizer depending on the model.
  It defaults to a cl100k_base tokenizer if the required tokenizer cannot be determined.
  
  It also performs caching of the tokenization results"""

  def __init__(self, model: RunnableModel) -> None:
    log.info(f'Creating tokenizer for model {model._id}')

    # creates the name used for accessing HF repositories
    if model.owner != '':
      hf_model_name = model.owner + '/' + model.name
    else:
      hf_model_name = model.name

    # Selects an appropriate tokenizer for HF models, selects a default tokenizer for other models
    if model.source == 'hf':
      log.info(f'Using HF tokenizer for {hf_model_name}')
      tokenizer = AutoTokenizer.from_pretrained(hf_model_name)
      tokenizer.encode_batch = lambda texts: [tokenizer.encode(text) for text in texts] # type: ignore
      tokenizer.decode_batch = lambda texts: [tokenizer.decode(text) for text in texts] # type: ignore
    else:
      DEFAULT_PROPRIETARY_TOKENIZER = 'cl100k_base'
      log.info(f'Using default tokenizer: {DEFAULT_PROPRIETARY_TOKENIZER}')
      tokenizer = tiktoken.get_encoding('cl100k_base')
    
    self._tokenizer = tokenizer
    self._cached_encodings = {}


  def _get_cached_encoding(self, text: Union[str, List[str]]) -> Union[List[int], List[List[int]], None]:
    """Returns already saved cached encoding, returns None if it wasnt cached"""

    # converts the list to tuple to allow dict lookup
    if isinstance(text, list):
      text = tuple(text)
    
    return self._cached_encodings.get(text, None)


  def _save_cached_encoding(self, text: Union[str, List[str]], encoding: Union[List[int], List[List[int]]]) -> None:
    """Saves cached encoding"""

    # converts the encoding to tuple to allow dict lookup
    if isinstance(text, list):
      text = tuple(text)
    
    self._cached_encodings[text] = encoding


  def encode(self, text: Union[str, List[str]]) -> Union[List[int], List[List[int]]]:
    """If the input wasn't cached, encodes the text and caches it for reuse"""

    cached_encoding = self._get_cached_encoding(text)
    if cached_encoding is not None:
      return cached_encoding

    if isinstance(text, str):
      encoded_result = self._tokenizer.encode(text)
    elif isinstance(text, list):
      encoded_result = self._tokenizer.encode_batch(text) # type: ignore
    else:
      raise TypeError(f'Not supported text type ({type(text)})')
    
    self._save_cached_encoding(text, encoded_result)
    return encoded_result
  

  def decode(
      self,
      encoded_text: Union[List[int], List[List[int]]],
      array_of_texts_was_passed: bool) -> Union[str, List[str]]:
    if array_of_texts_was_passed:
      return self._tokenizer.decode_batch(encoded_text) # type: ignore
    else:
      return self._tokenizer.decode(encoded_text) # type: ignore



class PromptAlterator:
  """Default class for prompt alteration strategies (like adding the appropriate instruction tokens around the prompt depending on the model). Returns the same thing as it's inputs are"""

  def start(self) -> str:
    """Returns a start of sentence token if applicable"""
    return ''
  
  def instruction(self, text: str) -> str:
    """Returns the text altered in such a way as to mark it as an instruction"""
    return text
  
  def normal_text(self, text: str) -> str:
    """Returns the text altered in such a way as to mark it as normal text""" 
    return text
  
  def assistant_text(self, text: str) -> str:
    """Returns the text altered in such a way as to mark it as generated by the AI""" 
    return text


class MistralAlterator(PromptAlterator):
  def start(self) -> str:
    return '<s>'
  
  def instruction(self, text: str) -> str:
    return ' '.join(['[INST]', text, '[/INST]'])
  

class Zephyr7BAlterator(PromptAlterator):
  def _format_tag_and_text(self, tag:str, text: str) -> Tuple[str, str]:
    """Since the tags are meant to be on newlines, this function removes a newline from the text (if there is one)
    and places it before the tag"""
    if text.startswith('\n'):
      return '\n' + tag, text[1:]
    else:
      return tag, text
  
  def instruction(self, text: str) -> str:
    """Converts the specified text into the following format:
    <|system|>
    text

    If the text starts with an newline, then that newline is removed and placed before the assistant tag
    """
    tag, text = self._format_tag_and_text('<|system|>', text)
    return '\n'.join([tag, text])
  
  def assistant_text(self, text: str) -> str:
    """Converts the specified text into the following format:
    <|assistant|>
    text

    If the text starts with an newline, then that newline is removed and placed before the assistant tag
    """
    tag, text = self._format_tag_and_text('<|assistant|>', text)
    return '\n'.join([tag, text])


def _get_prompt_alteration_mapping_for_model(model: RunnableModel) -> PromptAlterator:
  """Returns a propper prompt role marker for the model"""
  model_template_association = {
    ('mistral-7b-instruct',): MistralAlterator,
    ('zephyr-7b-beta',): Zephyr7BAlterator
  }
  selected_template = PromptAlterator
  for considered_template_model_names, prompt_template in model_template_association.items():
    for considered_template_model_name in considered_template_model_names:
      if considered_template_model_name in model.name:
        selected_template = prompt_template
        break
  return selected_template()

CONTEXT_SIZE_LEEWAY = 8

def _construct_default_reading_comprehension_prompt(
    context_text: str,
    question_dict: dict,
    model: RunnableModel,
    task_type: TaskType,
    tokenizer: UniversalTokenizer,
    prompt_alterator: PromptAlterator,
    **kwargs) -> Tuple[str, int, int]:
  """Creates a default RC prompt, matching for the model
  
  Returns the prompt, token count, and the number of tokens it was cut by"""

  question_text = question_dict['question']
  options_text = ''
  for i, option_text in enumerate(question_dict['options']):
    letter = idx2alphabet[i]
    options_text += f'{letter}) {option_text}\n'
  options_text = options_text.strip()

  # compute the token size for all the necessary text (header, question, options, suffic),
  # then cull the context text if necessary

  header_text = 'Read the text and answer the question correctly\n'
  questions_text = '\nQuestion: ' + question_text
  options_text = '\nOptions:\n' + options_text
  suffix_text = '\nThe correct answer is the letter:'
  encoded_necessary_text = tokenizer.encode([header_text, questions_text, options_text, suffix_text]) # type: List[List[int]]
  necessary_text_token_count = sum([len(tokens) for tokens in encoded_necessary_text]) + CONTEXT_SIZE_LEEWAY
  assert necessary_text_token_count < model.context_size, f'Necessary text ({necessary_text_token_count}) is larger than model context ({model.context_size})'

  tokenized_context = tokenizer.encode(context_text)
  max_tokens_after_generation = necessary_text_token_count + len(tokenized_context) + new_tokens_limit_per_task_type_int[task_type]
  exceeded_context_size_by = max(max_tokens_after_generation - model.context_size, 0)
  if exceeded_context_size_by > 0:
    log.warning(f'Cutting down RC text by {exceeded_context_size_by + 1} tokens (1 extra to add a "..." to the end)')
    context_text = tokenizer.decode(
      tokenized_context[:-(exceeded_context_size_by + 1)],
      array_of_texts_was_passed=False) + '...'

  final_text = (prompt_alterator.start() +
                prompt_alterator.instruction(''.join([header_text, context_text, questions_text, options_text])) +
                prompt_alterator.assistant_text(suffix_text))
  return final_text, max_tokens_after_generation, exceeded_context_size_by


def _construct_default_science_questions_prompt(
    question_dict: dict,
    model: RunnableModel,
    tokenizer: UniversalTokenizer,
    prompt_alterator: PromptAlterator,
    **kwargs) -> Tuple[str, int, int]:
  """Creates a default Science Quesions prompt, appropriate for the model
  
  Returns the prompt, token count, and the number of tokens it was cut by
  
  Unlike Reading Comprehension prompt constructor, this one will be using 1/2/3/4 answer labels, instead of A/B/C/D, for variety"""

  header_text = 'Read the question and choose the digit that corresponds to the correct answer'
  question_text = '\nQuestion: ' + question_dict['question']
  options_text = '\nPossible answer options:\n' + '\n'.join([
    f'Answer #{option_index+1}) {question_dict[option_label]}'
    for option_index, option_label in enumerate(['option1', 'option2', 'option3', 'option4'])])
  suffix_text = '\nThe digit that corresponds to the correct answer: #'

  # this prompt should be small enough for any model. In case where the prompt exceeds the allowed size - just crash
  encoded_text = tokenizer.encode([header_text, question_text, options_text, suffix_text]) # type: List[List[int]]
  token_count = sum([len(tokens) for tokens in encoded_text]) + CONTEXT_SIZE_LEEWAY
  assert token_count < model.context_size, f'Text ({token_count}) is larger than model context ({model.context_size})'
  max_tokens_after_generation = token_count + new_tokens_limit_per_task_type_int[TaskType.SCIENCE_QUESTIONS]
  exceeded_context_size_by = 0

  final_text = (prompt_alterator.start() +
                prompt_alterator.instruction(''.join([header_text, question_text, options_text])) +
                prompt_alterator.assistant_text(suffix_text))
  return final_text, max_tokens_after_generation, exceeded_context_size_by


def _construct_default_bot_detection_prompt(
    post_history: List[str],
    model: RunnableModel,
    task_type: TaskType,
    tokenizer: UniversalTokenizer,
    prompt_alterator: PromptAlterator,
    config: Dict,
    **kwargs) -> Tuple[str, int, int]:
  """Creates a default bot detection prompt, matching for the model
  
  Returns the prompt, token count, and the number of posts it was cut by"""

  introduction_text = """You are a helpful bot detection program. You will read the posts made by a user, and determine if they were made by a bot (Y/N)."""
  explaination_text = """ Posts made by a bot may usually involve advertisements (including with links), repeated posts, news titles, or they sound too monotonous."""
  examples_text = """\nHere are the examples:
Post: Airline employee steals plane from Seattle airport, crashes and dies - CNN: CNN Airline employee steals plane from Seattle airport, crashes and dies CNN (CNN) An airline employee stole an otherwise unoccupied passenger plane Friday from the\u2026 https://t.co/2FbjpYYuUH https://t.co/gtvQKqz4YG
Was this written by a bot?: "Y"
Post: Late night video editing. Finishing it up pre' soon. Hopefully I'll be able to put it up on YouTube inâ€¦ http://t.co/9OtxogubwQ
Was this written by a bot?: "N"
Post: It's highly unlikely, but I'd love if @andy_murray beat Djokovic here. As soon as he's back Murray should take a medical time out!
Was this written by a bot?: "N"
Post: Children at Play, ca. 1895-1897 https://t.co/P9Mpn4TNYS https://t.co/MJR7laIwcM
Was this written by a bot?: "Y"
Here is the user's post history:"""

  using_explaination = config.get('prompt_type', 'without explaination') == 'with explaination'
  if using_explaination:
    header_text = introduction_text + explaination_text + examples_text
  else:
    header_text = introduction_text + examples_text

  suffix_text = '\nWere these posts made by a bot?: "'
  encoded_necessary_text = tokenizer.encode([header_text, suffix_text])
  necessary_text_token_count = (sum([len(tokens) for tokens in encoded_necessary_text]) +
                                CONTEXT_SIZE_LEEWAY +
                                new_tokens_limit_per_task_type_int[task_type])
  max_allowed_tokens_for_posts = model.context_size - necessary_text_token_count

  '''
  Posts text should look like this:
  Post 1) some text some text some text
  Post 2) some text some text some text
  '''
  posts_current_token_count = 0
  posts_to_add = []
  for i, post in enumerate(post_history[:16]):
    post_text_to_add = f'\nPost {i+1}) {post}'
    post_token_size = len(tokenizer.encode(post_text_to_add))
    posts_new_token_count = posts_current_token_count + post_token_size
    if posts_new_token_count < max_allowed_tokens_for_posts:
      posts_to_add.append(post_text_to_add)
      posts_current_token_count = posts_new_token_count
  total_posts_text = ''.join(posts_to_add)

  final_text = (prompt_alterator.start() +
                prompt_alterator.instruction(''.join([header_text, total_posts_text])) +
                prompt_alterator.assistant_text(suffix_text))
  total_token_count = necessary_text_token_count + posts_current_token_count
  cut_posts = len(post_history) - len(posts_to_add)

  return final_text, total_token_count, cut_posts


NAME_OF_MULTIPLICATION_PROMPT_WITH_EXAMPLES = 'with examples'
def _construct_multiplication_prompt(
    math_expression: str,
    config: Dict,
    tokenizer: UniversalTokenizer,
    prompt_alterator: PromptAlterator,
    **kwargs) -> Tuple[str, int, int]:
  """Creates basic math prompt (with or without examples)"""
  instruction_string = 'Correctly solve the following math problems:'

  # decides whether or not to add examples
  examples ="""\n834*12=10008
942*105=98910
17*23=391
48*78=3744
266*19=5054
659*943=621437
4*22=88
873*246=214758
136*57=7752
18*18=324"""

  assistant_strings = []
  prompt_type = config.get('prompt_type', 'default')
  if prompt_type == NAME_OF_MULTIPLICATION_PROMPT_WITH_EXAMPLES:
    assistant_strings.append(examples)
  
  assistant_strings.append('\n' + math_expression)

  final_text = (prompt_alterator.start() +
                prompt_alterator.instruction(instruction_string) +
                prompt_alterator.assistant_text(''.join(assistant_strings)))
  total_token_count = len(tokenizer.encode(final_text))

  return final_text, total_token_count, 0


PROMPT_CONSTRUCTORS_MAP = { # maps task type and prompt type to constructor functions
  TaskType.READING_COMPREHENSION: {
    'default': _construct_default_reading_comprehension_prompt
  },
  TaskType.BOT_DETECTION: {
    'default': _construct_default_bot_detection_prompt,
    'without explaination': _construct_default_bot_detection_prompt,
    'with explaination': _construct_default_bot_detection_prompt
  },
  TaskType.MULTIPLICATION: {
    'default': _construct_multiplication_prompt,
    NAME_OF_MULTIPLICATION_PROMPT_WITH_EXAMPLES: _construct_multiplication_prompt,
    'without examples': _construct_multiplication_prompt
  },
  TaskType.SCIENCE_QUESTIONS: {
    'default': _construct_default_science_questions_prompt
  }
}


class PromptConstructor:
  """This class selects the appropriate prompt constructor based on task type and config, and forwards prompt constructor arguments to it when construct_prompt is called
  """

  def __init__(
      self,
      task_type: TaskType,
      configuration_dict: dict,
      model: RunnableModel,
      existing_tokenizer: Union[UniversalTokenizer, None] = None) -> None:
    self.model = model
    self.task_type = task_type
    self.configuration_dict = configuration_dict

    if existing_tokenizer is None:
      self.tokenizer = UniversalTokenizer(model)
    else:
      self.tokenizer = existing_tokenizer

    self.prompt_alterator = _get_prompt_alteration_mapping_for_model(self.model)
    log.info(f'Decided on the following prompt alterator: {type(self.prompt_alterator)}')
    self.constructor_function = self._get_prompt_constructor_function() # type: Callable


  def construct_prompt(self, /, **kwargs) -> Tuple[str, int, int]:
    """Kwargs guide per task type:
    - Reading comprehension:
    - - context_text: str
    - - question_dict: dict
    - Bot Detection:
    - - post_history: List[str]
    - Basic Math:
    - - math_expression: str
    - Science Questions:
    - - question_dict: dict

    Returns the prompt text, token count, as well as the number of tokens it was cut by
    """
    return self.constructor_function(
      model=self.model,
      prompt_alterator=self.prompt_alterator,
      task_type=self.task_type,
      tokenizer=self.tokenizer,
      config=self.configuration_dict,
      **kwargs)
  
  
  def _get_prompt_constructor_function(self) -> Callable[..., Tuple[str, int, int]]:
    prompt_type = self.configuration_dict.get('prompt_type', 'default')
    log.info(f'Finding the prompt constructor function for prompt type {prompt_type} and task {self.task_type}')

    prompt_constructors_for_task = PROMPT_CONSTRUCTORS_MAP.get(self.task_type, None) # type: Union[dict, None]
    if prompt_constructors_for_task is None:
      raise AttributeError(f'Could not find prompt constructors for task {self.task_type}')

    prompt_constructor_for_task_and_prompt_type = prompt_constructors_for_task.get(prompt_type, None) # type: Union[Callable, None]
    if prompt_constructor_for_task_and_prompt_type is None:
      raise AttributeError(f'Could not find prompt constructor for task {self.task_type} and prompt type {prompt_type}')
    
    log.info(f'Found prompt constructor named {prompt_constructor_for_task_and_prompt_type.__name__}')

    return prompt_constructor_for_task_and_prompt_type
    