<div>
  <div id="task_details">
    <b><label>Input code: "{{input_code}}", equation: "{{equation}}"</label></b>
    <br/>
    <labe>The correct answer: {{answer}}</label>
    <br/>
  </div>
  <b>Test results for specific LLM-configuration combinations:</b>
  <div id="results_per_model">
    {% for readable_name, prompt_and_answers in prompt_and_interpreted_output_counts_per_readable_llm_config_combination.items %}
      {% if prompt_and_answers.answers|length > 0 %}
        <label>Outputs for this test for model {{readable_name}}:</label>
        <pre><p>Prompt:{{prompt_and_answers.prompt}}</p></pre>
        <ul>
        {% for answer in prompt_and_answers.answers %}
          <li>Answer #{{ forloop.counter }}: "{{ answer.model_output }}" (interpreted as "{{ answer.interpreted_output }}")</li>
        {% endfor %}
        </ul>
        {% else %}
        <label>There are no outputs for model {{readable_name}}</label>
      {% endif %}
    {% endfor %}
  </div>
</div>
