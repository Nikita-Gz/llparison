<div>
  <div id="task_details">
    <b><label>Question (input code = {{input_code}}): </label></b>
    <p>{{ question }}</p>
    <label>Answer options:</label>
    <ul>
    {% for option in options %}
      <li>Answer #{{forloop.counter}}) {{option}}</li>
    {% endfor %}
    </ul>
    <labe>Correct answer: #{{answer_index}}</label>
    <br/>
  </div>
  <b>Specific model results:</b>
  <div id="results_per_model">
    {% for readable_name, prompt_and_answers in prompt_and_interpreted_output_counts_per_readable_llm_config_combination.items %}
      {% if prompt_and_answers.answers|length > 0 %}
        <label>Outputs for this test for model {{readable_name}}:</label>
        <pre><p>Prompt:{{prompt_and_answers.prompt}}</p></pre>
        <ul>
        {% for answer in prompt_and_answers.answers %}
          <li>Answer {{ forloop.counter }}: "{{ answer.model_output }}" (interpreted as "{{ answer.interpreted_output }}")</li>
        {% endfor %}
        </ul>
        {% else %}
        <label>There are no outputs for model {{readable_name}}</label>
      {% endif %}
    {% endfor %}
  </div>
</div>
